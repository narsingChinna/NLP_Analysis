{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4i9-vQuR1my"
   },
   "outputs": [],
   "source": [
    "#preprocessing of dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('Survey Feedback or Open-Ended Responses for airlines.csv')\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT0muFLBTHZz"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNDw4I2iTOw9"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNuTDIwHTikk"
   },
   "outputs": [],
   "source": [
    "#Finding the null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVgoshLoUCEC"
   },
   "outputs": [],
   "source": [
    "#null value percentage\n",
    "df.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGmgv6BHUjWn"
   },
   "outputs": [],
   "source": [
    "#Drop the columns\n",
    "\n",
    "\n",
    "df.drop(columns=['negativereason_gold'], inplace=True)\n",
    "df.drop(columns=['tweet_coord'], inplace=True)\n",
    "df.drop(columns=['airline_sentiment_gold'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1raGzfPWjOI"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMFwYY6TWznY"
   },
   "outputs": [],
   "source": [
    "#Finding the null values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRgewR-5YRgn"
   },
   "outputs": [],
   "source": [
    "#Fill the null values using the mode method\n",
    "df['negativereason'].fillna(df['negativereason'].mode()[0], inplace=True)\n",
    "df['negativereason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhmGPGloY8ZC"
   },
   "outputs": [],
   "source": [
    "#filling the null values using the mode method\n",
    "df['negativereason_confidence'].fillna(df['negativereason_confidence'].mode()[0], inplace=True)\n",
    "df['negativereason_confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUcaQPjzZaea"
   },
   "outputs": [],
   "source": [
    "#filling the null values using the mode method\n",
    "df['tweet_location'].fillna(df['tweet_location'].mode()[0], inplace=True)\n",
    "df['tweet_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeDWl40xZ7Ep"
   },
   "outputs": [],
   "source": [
    "#filling  the null vslues using the mode method\n",
    "df['user_timezone'].fillna(df['user_timezone'].mode()[0], inplace=True)\n",
    "df['user_timezone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O92v1ZRJbxAd"
   },
   "source": [
    "Group by analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i4t7kUadcCN"
   },
   "source": [
    "How you can perform a group by analysis to find, for example, the count of negative reasons for each airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzaF7qQ8aZmw"
   },
   "outputs": [],
   "source": [
    "#How you can perform a group by analysis to find, for example, the count of negative reasons for each airline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your dataframe is named 'df'\n",
    "\n",
    "# 1. Group by 'airline' and 'negativereason'\n",
    "grouped = df.groupby(['airline', 'negativereason'])\n",
    "\n",
    "# 2. Aggregate using count() to get the number of occurrences\n",
    "result = grouped.size().reset_index(name='counts')\n",
    "\n",
    "# 3. Display the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbIm1CmXdUrX"
   },
   "source": [
    "perform a group by analysis using airline_sentiment and the rest of the columns in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkqjUWcIfS6E"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "grouped = df.groupby('airline_sentiment')\n",
    "result = grouped.agg({\n",
    "       'tweet_id': 'count',             # Count of tweets for each sentiment\n",
    "       'airline': lambda x: x.mode()[0], # Most frequent airline for each sentiment\n",
    "       'negativereason': lambda x: x.mode()[0],  # Most frequent negative reason\n",
    "       'negativereason_confidence': 'mean', # Average confidence for negative reasons\n",
    "       'retweet_count': 'sum',          # Total retweets for each sentiment\n",
    "       'tweet_location': lambda x: x.mode()[0],  # Most frequent tweet location\n",
    "       'user_timezone': lambda x: x.mode()[0]   # Most frequent user timezone\n",
    "       # Add aggregations for other numerical or categorical columns as needed\n",
    "   })\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QigQYFpRfmPp"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0bNnyj1gVNY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Value counts for target variable (airline_sentiment)\n",
    "target_counts = df['airline_sentiment'].value_counts()\n",
    "print(\"Value Counts for Target Variable (airline_sentiment):\\n\", target_counts)\n",
    "\n",
    "# Value counts for other columns\n",
    "for column in df.columns:\n",
    "    if column != 'airline_sentiment':\n",
    "        column_counts = df[column].value_counts()\n",
    "        print(f\"\\nValue Counts for {column}:\\n\", column_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwwMP63glMTr"
   },
   "source": [
    "#Finding the value count for the target variable ['airline_sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db1Wsv48hdpm"
   },
   "outputs": [],
   "source": [
    "#Finding the value count for the targert vsriable ['airline_sentiment']\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get value counts for the target variable\n",
    "target_counts = df['airline_sentiment'].value_counts()\n",
    "\n",
    "# Create a bar plot using Seaborn\n",
    "plt.figure(figsize=(8, 6))  # Adjust figsize as needed\n",
    "sns.countplot(x='airline_sentiment', data=df)\n",
    "plt.title('Distribution of Airline Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5tdgMXKjtdi"
   },
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable (airline_sentiment)\n",
    "df['airline_sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoO-yjxP5Uxi"
   },
   "source": [
    "Balancing the Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icjc5c8369bI"
   },
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'df' that you want to balance\n",
    "# You would first need to perform the balancing operation\n",
    "# Here's an example using the 'imblearn' library (install if needed)\n",
    "\n",
    "!pip install imblearn\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# ... (your previous code to load and preprocess 'df') ...\n",
    "\n",
    "# Assuming 'airline_sentiment' is your target column\n",
    "X = df.drop('airline_sentiment', axis=1)  # Features\n",
    "y = df['airline_sentiment']              # Target\n",
    "\n",
    "# Create an instance of the RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state=42)  # Adjust random_state as needed\n",
    "\n",
    "# Resample the data\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Create the balanced DataFrame\n",
    "balanced_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "balanced_df['airline_sentiment'] = y_resampled\n",
    "\n",
    "# Now you can use 'balanced_df' as intended\n",
    "value_counts_after_balancing = balanced_df['airline_sentiment'].value_counts()\n",
    "\n",
    "print(\"Value Counts of 'airline_sentiment' After Balancing:\\n\", value_counts_after_balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGEh1F9-kq_Y"
   },
   "outputs": [],
   "source": [
    "# Assuming 'balanced_df' is your balanced DataFrame\n",
    "\n",
    "value_counts_after_balancing = balanced_df['airline_sentiment'].value_counts()\n",
    "\n",
    "print(\"Value Counts of 'airline_sentiment' After Balancing:\\n\", value_counts_after_balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuSxOt5hky8k"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'balanced_df' is your balanced DataFrame\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Adjust figsize as needed\n",
    "sns.countplot(x='airline_sentiment', data=balanced_df)\n",
    "plt.title('Distribution of Airline Sentiment (After Balancing)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clg3cmZUmPwU"
   },
   "source": [
    "we need to perform NLP on tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVrcOzpLzLZd"
   },
   "source": [
    "Perform tokenization for the Text Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETGuK3_cy20I"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the 'punkt_tab' resource\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Now apply word_tokenize\n",
    "df['tokenized_text'] = df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB5GFByKzaVi"
   },
   "source": [
    "After performing Tokenization  New Column tokenized_text was Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVY3ChdrzD1_"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqC0vdN0zDvO"
   },
   "outputs": [],
   "source": [
    "#stemming and lemmatization for the tokenized_text\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z22LsaSm1u9J"
   },
   "source": [
    "#performing the lemmatization for tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFDm85dx1QlM"
   },
   "outputs": [],
   "source": [
    "#performing the lemmatization for tokenized Text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') # Download for WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens: A list of tokens (words).\n",
    "\n",
    "    Returns:\n",
    "        A list of lemmatized tokens.\n",
    "    \"\"\"\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token,pos=\"v\") for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Apply the lemmatization function to the 'tokenized_text' column\n",
    "df['lemmatized_text'] = df['tokenized_text'].apply(lemmatize_tokens)\n",
    "df['lemmatized_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBbgTC5k7oOE"
   },
   "outputs": [],
   "source": [
    "#After lemmatization lemmatized_text added to dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Do7IFbA2YlM"
   },
   "source": [
    "#Performing the Regular expression to clean  and Preprocess the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8y_YNRSw1xbx"
   },
   "outputs": [],
   "source": [
    "#Performing the Regular expression to clean and Preprocess the text data\n",
    "import re\n",
    "\n",
    "def clean_text_with_regex(text):\n",
    "    \"\"\"Cleans text using regular expressions.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned text string.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "\n",
    "    # Remove punctuation and special characters (keep only letters, numbers, and spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'text' column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text_with_regex)\n",
    "df['cleaned_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYIqNhWCAhwc"
   },
   "source": [
    "#Removing the Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jO9o4S7F2jeK"
   },
   "outputs": [],
   "source": [
    "#Removing the Stop Words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')  # Download stopwords if you haven't already\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Removes stop words from a text string.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "        The text string with stop words removed.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)  # Join the filtered words back into a string\n",
    "\n",
    "# Apply the remove_stopwords function to the 'cleaned_text' column\n",
    "df['text_without_stopwords'] = df['cleaned_text'].apply(remove_stopwords)\n",
    "df['text_without_stopwords']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvPy6w74Abou"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtsJVlwR4HDK"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_Pzt1i4Ax5P"
   },
   "source": [
    "Using Fast text : leading to faster training and smaller model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrLh86Bzkr3A"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Prepare the data for FastText\n",
    "# Assuming 'text_without_stopwords' column contains the preprocessed text\n",
    "# and 'airline_sentiment' column contains the labels\n",
    "\n",
    "# Filter out empty or very short text samples\n",
    "df_filtered = df[df['text_without_stopwords'].str.strip() != \"\"]\n",
    "df_filtered = df_filtered[df_filtered['text_without_stopwords'].str.split().str.len() >= 3]  # Keep samples with at least 3 words\n",
    "\n",
    "\n",
    "# Create a training file in FastText format\n",
    "with open('train.txt', 'w') as f:\n",
    "    for index, row in df_filtered.iterrows():  # Iterate through the filtered DataFrame\n",
    "        text = row['text_without_stopwords']\n",
    "        label = row['airline_sentiment']\n",
    "        f.write(f\"__label__{label} {text}\\n\")\n",
    "\n",
    "# Train the FastText model with adjusted hyperparameters\n",
    "model = fasttext.train_supervised(\n",
    "    input=\"train.txt\",\n",
    "    lr=0.1,  # Reduced learning rate\n",
    "    epoch=25,  # Adjust epoch if needed\n",
    "    wordNgrams=2,  # Adjust wordNgrams if needed\n",
    "    loss='hs'  # Adjust loss function if needed\n",
    ")\n",
    "\n",
    "# Save the model for later use (optional)\n",
    "model.save_model(\"fasttext_model.bin\")\n",
    "\n",
    "# Make predictions on new data\n",
    "# Example:\n",
    "new_text = \"This airline is terrible, the flight was delayed.\"\n",
    "prediction = model.predict(new_text)\n",
    "print(prediction)  # Output: [(('__label__negative',), 0.9999998807907104)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMZ9Pqn3LV2Z"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6muf0J2L1-N"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "# Assuming df['tokenized_text'] or df['lemmatized_text'] contains your preprocessed, tokenized text data\n",
    "# Choose the appropriate column based on your preprocessing steps\n",
    "model_embedding = FastText(sentences=df['tokenized_text'], vector_size=300, window=5, min_count=5, sg=1)\n",
    "model_embedding\n",
    "\n",
    "# Or\n",
    "# model_embedding = FastText(sentences=df['lemmatized_text'], vector_size=300, window=5, min_count=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VkFtE354k3C"
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Prepare the data for FastText\n",
    "# Assuming 'text_without_stopwords' column contains the preprocessed text\n",
    "# and 'airline_sentiment' column contains the labels\n",
    "\n",
    "# Filter out empty or very short text samples\n",
    "df_filtered = df[df['text_without_stopwords'].str.strip() != \"\"]\n",
    "df_filtered = df_filtered[df_filtered['text_without_stopwords'].str.split().str.len() >= 3]  # Keep samples with at least 3 words\n",
    "\n",
    "\n",
    "# Create a training file in FastText format\n",
    "with open('train.txt', 'w') as f:\n",
    "    for index, row in df_filtered.iterrows():  # Iterate through the filtered DataFrame\n",
    "        text = row['text_without_stopwords']\n",
    "        label = row['airline_sentiment']\n",
    "        f.write(f\"__label__{label} {text}\\n\")\n",
    "\n",
    "# Train the FastText model with adjusted hyperparameters\n",
    "model = fasttext.train_supervised(\n",
    "    input=\"train.txt\",\n",
    "    lr=0.1,  # Reduced learning rate\n",
    "    epoch=25,  # Adjust epoch if needed\n",
    "    wordNgrams=2,  # Adjust wordNgrams if needed\n",
    "    loss='hs'  # Adjust loss function if needed\n",
    ")\n",
    "\n",
    "# Save the model for later use (optional)\n",
    "model.save_model(\"fasttext_model.bin\")\n",
    "\n",
    "# Make predictions on new data\n",
    "# Example:\n",
    "new_text = \"This airline is terrible, the flight was delayed.\"\n",
    "prediction = model.predict(new_text)\n",
    "print(prediction)  # Output: [(('__label__negative',), 0.9999998807907104)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJwoP2geAN0-"
   },
   "outputs": [],
   "source": [
    "# Function to get the average word vector for a document\n",
    "def get_avg_word_vector(tokens):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if not vectors:\n",
    "        # If no in-vocabulary words are present, return a default vector (zeros in this case)\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return sum(vectors) / len(vectors) # vector representation of sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E54_TKw-D_ir"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zGq1SC3pO0TG"
   },
   "outputs": [],
   "source": [
    "df['text_without_stopwords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBtxwj2dVt7Z"
   },
   "source": [
    "Convert the text  into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo7xRpY0VHlr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the 'punkt' resource if you haven't already\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to get the average word vector for a document\n",
    "def get_avg_word_vector(tokens):\n",
    "    \"\"\"Gets the average word vector for a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens: A list of tokens (words).\n",
    "\n",
    "    Returns:\n",
    "        The average word vector (numpy array).\n",
    "    \"\"\"\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]\n",
    "    if not vectors:\n",
    "        # If no words are present, return a default vector (zeros in this case)\n",
    "        return np.zeros(model.get_dimension())\n",
    "    else:\n",
    "        return sum(vectors) / len(vectors)  # Vector representation of the sentence\n",
    "\n",
    "def sentence_to_vector(sentence):\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)  # Tokenize the sentence\n",
    "    vector = get_avg_word_vector(tokens)  # Get the average word vector using the modified function\n",
    "    return vector\n",
    "\n",
    "# Example usage:\n",
    "new_sentence = \"This airline is great, the flight was on time.\"\n",
    "sentence_vector = sentence_to_vector(new_sentence)\n",
    "print(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_56R-YpSh1RS"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pandas as pd  # Assuming you have your data in a pandas DataFrame 'df'\n",
    "\n",
    "# Combine all text data into a single string\n",
    "all_text = ' '.join(df['text_without_stopwords'].astype(str).tolist())\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    stopwords=STOPWORDS,  # Use default stopwords\n",
    "    max_words=200,  # Adjust as needed\n",
    "    min_font_size=10,  # Adjust as needed\n",
    ").generate(all_text)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSb--0i4Q9OR"
   },
   "source": [
    "Sentiment Analysis Approach\n",
    "\n",
    "You want to take a sentence as input and analyze its sentiment (positive, neutral, or negative) using the same sentiment analysis approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZh-htAnQqr9",
    "outputId": "84133511-a762-4c8d-d61e-0ea2f58aaa54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Analyzes the sentiment of a text string.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "\n",
    "    Returns:\n",
    "        The sentiment label ('positive', 'neutral', or 'negative').\n",
    "    \"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    sentiment_polarity = analysis.sentiment.polarity\n",
    "\n",
    "    if sentiment_polarity > 0:\n",
    "        return 'positive'\n",
    "    elif sentiment_polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Get input sentence from the user\n",
    "input_sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "# Analyze the sentiment of the input sentence\n",
    "sentiment = analyze_sentiment(input_sentence)\n",
    "\n",
    "# Print the sentiment\n",
    "print(f\"The sentiment of the sentence is: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0rkad8NZj16"
   },
   "source": [
    "perform logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC27Lz_XY22G"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence):\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)  # Tokenize the sentence\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]  # Get word vectors\n",
    "    if vectors:  # If any word vectors were found\n",
    "        return sum(vectors) / len(vectors)  # Return the average word vector\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())  # Return a zero vector if no word vectors found\n",
    "\n",
    "# 3. Create feature matrix and target vector\n",
    "X = df['text_without_stopwords'].apply(sentence_to_vector).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the logistic regression model with regularization\n",
    "model_lr = LogisticRegression(max_iter=1000, penalty='l2', C=1.0)  # Added regularization\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "y_pred_test = model_lr.predict(X_test)\n",
    "\n",
    "# 7. Make predictions on the training set\n",
    "y_pred_train = model_lr.predict(X_train)\n",
    "\n",
    "# 8. Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# 9. Evaluate the model on the training set\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "# Print classification report for the test set\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmGCzRHekwsb"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence):\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)  # Tokenize the sentence\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]  # Get word vectors\n",
    "    if vectors:  # If any word vectors were found\n",
    "        return sum(vectors) / len(vectors)  # Return the average word vector\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())  # Return a zero vector if no word vectors found\n",
    "\n",
    "# 3. Create feature matrix and target vector\n",
    "X = df['text_without_stopwords'].apply(sentence_to_vector).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the logistic regression model with regularization and multi_class parameter\n",
    "model_lr = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    multi_class='multinomial'  # Handle multi-class classification\n",
    ")\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# 6. Train XGBoost model\n",
    "# Encode the target variable for XGBoost (if it's not numerical)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "xgb_classifier = XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "xgb_classifier.fit(X_train, y_train_encoded)  # Use encoded target for training\n",
    "\n",
    "\n",
    "# 7. Make predictions\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# 8. Evaluate Logistic Regression\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_lr)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_lr)}\")\n",
    "\n",
    "# 9. Evaluate XGBoost\n",
    "print(\"\\nXGBoost:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_encoded, y_pred_xgb):.4f}\")  # Use encoded target for evaluation\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_encoded, y_pred_xgb, target_names=label_encoder.classes_)}\")  # Use original labels\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test_encoded, y_pred_xgb)}\")  # Use encoded target for confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yP-TYuEbeDM"
   },
   "source": [
    "Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxXCQ5j_AIKZ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np  # Make sure numpy is imported\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())  # Assuming 'model' is your FastText model\n",
    "\n",
    "# 3. Create feature matrix and target vector\n",
    "# Assuming 'df' is your DataFrame with 'text_without_stopwords' and 'airline_sentiment' columns\n",
    "X = df['text_without_stopwords'].apply(sentence_to_vector).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the Decision Tree Classifier with hyperparameter tuning\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=2,  # Restrict depth for simplicity\n",
    "    min_samples_split=100,  # Require many samples to split\n",
    "    min_samples_leaf=50,  # Require many samples in a leaf\n",
    ")\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 6. Make predictions\n",
    "y_pred_train = dt_classifier.predict(X_train)\n",
    "y_pred_test = dt_classifier.predict(X_test)\n",
    "\n",
    "# 7. Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# 8. Print results\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rar03hhW9AJk"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier  # Import XGBoost\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder #For label encoding\n",
    "\n",
    "# ... (your existing code for sentence_to_vector, X, y, train_test_split) ...\n",
    "\n",
    "# 5. Train Decision Tree (same as before)\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=2, min_samples_split=100, min_samples_leaf=50)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# --- XGBoost ---\n",
    "# 1. Encode target variable for XGBoost (if needed)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# 2. Train XGBoost\n",
    "xgb_classifier = XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# --- Evaluation ---\n",
    "# 1. Predictions\n",
    "y_pred_dt = dt_classifier.predict(X_test)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# 2. Decision Tree Evaluation\n",
    "print(\"Decision Tree:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_dt)}\")\n",
    "\n",
    "# 3. XGBoost Evaluation\n",
    "print(\"\\nXGBoost:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_encoded, y_pred_xgb):.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test_encoded, y_pred_xgb, target_names=label_encoder.classes_)}\")\n",
    "\n",
    "# 4. Confusion Matrix (for both)\n",
    "print(\"\\nConfusion Matrix (Decision Tree):\\n\", confusion_matrix(y_test, y_pred_dt))\n",
    "print(\"\\nConfusion Matrix (XGBoost):\\n\", confusion_matrix(y_test_encoded, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRts_iFaeVaq"
   },
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "al-cIPyweIrY"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence):\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)  # Tokenize the sentence\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]  # Get word vectors\n",
    "    if vectors:  # If any word vectors were found\n",
    "        return sum(vectors) / len(vectors)  # Return the average word vector\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())  # Return a zero vector if no word vectors found\n",
    "\n",
    "# 3. Create feature matrix and target vector\n",
    "X = df['text_without_stopwords'].apply(sentence_to_vector).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the Random Forest Classifier with more aggressive hyperparameter tuning\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,  # You might try increasing this, but be cautious of computation time\n",
    "    random_state=42,\n",
    "    max_depth=5,  # Further reduce max_depth\n",
    "    min_samples_split=10,  # Increase min_samples_split\n",
    "    min_samples_leaf=5,  # Increase min_samples_leaf\n",
    "    max_features='sqrt',  # You can also experiment with other values, like 'log2' or an integer\n",
    "    # Consider adding:\n",
    "    # bootstrap=False,  # Disable bootstrapping (bagging) - can sometimes help\n",
    "    # oob_score=True,  # Use out-of-bag samples for evaluation\n",
    ")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 6. Make predictions and evaluate (same as before)\n",
    "#Add the prediction and evaluation code here\n",
    "y_pred_test = rf_classifier.predict(X_test)\n",
    "y_pred_train = rf_classifier.predict(X_train)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfCPLvw2vITu"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier  # Import for XGBoost\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder  # Import for Label Encoding\n",
    "\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence):\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)  # Tokenize the sentence\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]  # Get word vectors\n",
    "    if vectors:  # If any word vectors were found\n",
    "        return sum(vectors) / len(vectors)  # Return the average word vector\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())  # Return a zero vector if no word vectors found\n",
    "\n",
    "# 3. Create feature matrix and target vector\n",
    "X = df['text_without_stopwords'].apply(sentence_to_vector).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the Random Forest Classifier with more aggressive hyperparameter tuning\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,  # You might try increasing this, but be cautious of computation time\n",
    "    random_state=42,\n",
    "    max_depth=5,  # Further reduce max_depth\n",
    "    min_samples_split=10,  # Increase min_samples_split\n",
    "    min_samples_leaf=5,  # Increase min_samples_leaf\n",
    "    max_features='sqrt',  # You can also experiment with other values, like 'log2' or an integer\n",
    "    # Consider adding:\n",
    "    # bootstrap=False,  # Disable bootstrapping (bagging) - can sometimes help\n",
    "    # oob_score=True,  # Use out-of-bag samples for evaluation\n",
    ")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# --- XGBoost and Evaluation ---\n",
    "\n",
    "# 1. Train XGBoost model\n",
    "label_encoder = LabelEncoder()  # Create a LabelEncoder object\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode training labels\n",
    "y_test_encoded = label_encoder.transform(y_test)  # Encode test labels\n",
    "\n",
    "xgb_classifier = XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# 2. Make predictions for Random Forest and XGBoost\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# 3. Evaluate Random Forest and XGBoost\n",
    "# Accuracy\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "xgb_accuracy = accuracy_score(y_test_encoded, y_pred_xgb)  # Use encoded y_test for XGBoost\n",
    "print(f\"Random Forest Test Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"XGBoost Test Accuracy: {xgb_accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix for Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"\\nConfusion Matrix (Random Forest):\\n\", cm_rf)\n",
    "\n",
    "# Classification Report for Random Forest\n",
    "print(\"\\nClassification Report (Random Forest):\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier  # Import for XGBoost\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder  # Import for Label Encoding\n",
    "\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence):\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)  # Tokenize the sentence\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]  # Get word vectors\n",
    "    if vectors:  # If any word vectors were found\n",
    "        return sum(vectors) / len(vectors)  # Return the average word vector\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())  # Return a zero vector if no word vectors found\n",
    "\n",
    "# 3. Create feature matrix and target vector\n",
    "X = df['text_without_stopwords'].apply(sentence_to_vector).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the Random Forest Classifier with more aggressive hyperparameter tuning\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,  # You might try increasing this, but be cautious of computation time\n",
    "    random_state=42,\n",
    "    max_depth=5,  # Further reduce max_depth\n",
    "    min_samples_split=10,  # Increase min_samples_split\n",
    "    min_samples_leaf=5,  # Increase min_samples_leaf\n",
    "    max_features='sqrt',  # You can also experiment with other values, like 'log2' or an integer\n",
    "    # Consider adding:\n",
    "    # bootstrap=False,  # Disable bootstrapping (bagging) - can sometimes help\n",
    "    # oob_score=True,  # Use out-of-bag samples for evaluation\n",
    ")\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# --- XGBoost and Evaluation ---\n",
    "\n",
    "# 1. Train XGBoost model\n",
    "label_encoder = LabelEncoder()  # Create a LabelEncoder object\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode training labels\n",
    "y_test_encoded = label_encoder.transform(y_test)  # Encode test labels\n",
    "\n",
    "xgb_classifier = XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# 2. Make predictions for Random Forest and XGBoost\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# 3. Evaluate Random Forest and XGBoost\n",
    "# Accuracy\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "xgb_accuracy = accuracy_score(y_test_encoded, y_pred_xgb)  # Use encoded y_test for XGBoost\n",
    "print(f\"Random Forest Test Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"XGBoost Test Accuracy: {xgb_accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix for Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"\\nConfusion Matrix (Random Forest):\\n\", cm_rf)\n",
    "\n",
    "# Classification Report for Random Forest\n",
    "print(\"\\nClassification Report (Random Forest):\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Confusion Matrix for XGBoost\n",
    "cm_xgb = confusion_matrix(y_test_encoded, y_pred_xgb)  # Use encoded y_test for XGBoost\n",
    "print(\"\\nConfusion Matrix (XGBoost):\\n\", cm_xgb)\n",
    "\n",
    "# Classification Report for XGBoost\n",
    "print(\"\\nClassification Report (XGBoost):\\n\", classification_report(y_test_encoded, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTnLbA3AXFH6"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.2.2 xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKs081GyJQwN"
   },
   "source": [
    "perform confusion matrix ,accuracy of xg boost, and classification report of percision ,recall, f1score support for above random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuUWcDUxg6dL"
   },
   "source": [
    "Perform SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JansDoriZ0h0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC  # Import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming 'model' is your trained FastText model (from previous cell)\n",
    "# Make sure 'model' is defined and accessible in this cell\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence, model): # Pass the model as an argument\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)  # Tokenize the sentence\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]  # Get word vectors\n",
    "    if vectors:  # If any word vectors were found\n",
    "        return sum(vectors) / len(vectors)  # Return the average word vector\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())  # Return a zero vector if no word vectors found\n",
    "\n",
    "# 3. Create feature matrix and target vector\n",
    "# Pass the 'model' object to the 'sentence_to_vector' function\n",
    "X = df['text_without_stopwords'].apply(lambda sentence: sentence_to_vector(sentence, model)).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the SVM model (LinearSVC)\n",
    "svm_classifier = LinearSVC(random_state=42, max_iter=10000)  # Initialize LinearSVC\n",
    "svm_classifier.fit(X_train, y_train)  # Train the model\n",
    "\n",
    "# 6. Make predictions on the test set\n",
    "y_pred_test = svm_classifier.predict(X_test)\n",
    "\n",
    "# 7. Make predictions on the training set\n",
    "y_pred_train = svm_classifier.predict(X_train)\n",
    "\n",
    "# 8. Evaluate the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# 9. Evaluate the model on the training set\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "# Print classification report for the test set\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx6RiUVOJffC"
   },
   "source": [
    "perform confusion matrix ,accuracy of xg boost, and classification report of percision ,recall, f1score support for above SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRyvN32bxJ50"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming 'model' is your trained FastText model (from a previous cell)\n",
    "# Make sure 'model' is defined and accessible in this cell\n",
    "\n",
    "# 2. Function to convert a sentence into a vector using the FastText model\n",
    "def sentence_to_vector(sentence, model):\n",
    "    \"\"\"Converts a sentence into a vector using the FastText model.\n",
    "\n",
    "    Args:\n",
    "        sentence: The input sentence (string).\n",
    "        model: The trained FastText model.\n",
    "\n",
    "    Returns:\n",
    "        The vector representation of the sentence (numpy array).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    vectors = [model.get_word_vector(word) for word in tokens]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return np.zeros(model.get_dimension())\n",
    "\n",
    "# 3. Assuming 'df' is your DataFrame with 'text_without_stopwords' and 'airline_sentiment' columns\n",
    "# Make sure 'df' and 'model' are defined and accessible in this cell\n",
    "# Create feature matrix and target vector\n",
    "X = df['text_without_stopwords'].apply(lambda sentence: sentence_to_vector(sentence, model)).to_list()\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# 4. Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Train the SVM model (LinearSVC)\n",
    "svm_classifier = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 6. Train XGBoost model\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "xgb_classifier = XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "xgb_classifier.fit(X_train, y_train_encoded)\n",
    "\n",
    "# 7. Make Predictions for SVM and XGBoost\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# 8. Evaluate SVM and XGBoost\n",
    "# Accuracy\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "xgb_accuracy = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "print(f\"SVM Test Accuracy: {svm_accuracy:.4f}\")\n",
    "print(f\"XGBoost Test Accuracy: {xgb_accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(\"\\nConfusion Matrix (SVM):\\n\", cm_svm)\n",
    "\n",
    "# Classification Report for SVM\n",
    "print(\"\\nClassification Report (SVM):\\n\", classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Confusion Matrix for XGBoost\n",
    "cm_xgb = confusion_matrix(y_test_encoded, y_pred_xgb)\n",
    "print(\"\\nConfusion Matrix (XGBoost):\\n\", cm_xgb)\n",
    "\n",
    "# Classification Report for XGBoost\n",
    "print(\"\\nClassification Report (XGBoost):\\n\", classification_report(y_test_encoded, y_pred_xgb, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4aJqC7R6iKT"
   },
   "source": [
    "Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgSDkMNgma3C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have calculated the accuracy for Logistic Regression and SVM\n",
    "# and stored them in variables like 'logistic_regression_accuracy' and 'svm_accuracy'\n",
    "\n",
    "# Update model_data dictionary with new entries for Logistic Regression and SVM\n",
    "model_data = {\n",
    "    'Model': ['Decision Tree', 'Random Forest', 'Logistic Regression', 'XGBoost'],\n",
    "    'Training Accuracy': [87.05,87.63,86.85,87.16 ],  # Add accuracy for Logistic Regression and SVM\n",
    "    'Testing Accuracy': [87.26,88.43,86.75,87.16],  # Add accuracy for Logistic Regression and SVM\n",
    "    'Overall Accuracy': [87,88,87,87.16]  # Add accuracy for Logistic Regression and SVM\n",
    "}\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "# Creating a DataFrame and sorting\n",
    "accuracy_df = pd.DataFrame(model_data)\n",
    "accuracy_df_sorted = accuracy_df.sort_values(by='Overall Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display sorted table as text\n",
    "print(accuracy_df_sorted)\n",
    "\n",
    "# --- Table Visualization ---\n",
    "fig, ax = plt.subplots(figsize=(12, 4))  # Increased figsize for more models\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = [accuracy_df_sorted.columns.tolist()] + accuracy_df_sorted.values.tolist()\n",
    "table = ax.table(cellText=table_data, cellLoc='center', loc='center')\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "plt.title(\"Model Accuracy Comparison Grid\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# --- Heatmap Visualization ---\n",
    "plt.figure(figsize=(10, 5))  # Increased figsize for more models\n",
    "sns.heatmap(accuracy_df_sorted.set_index('Model'), annot=True, cmap='Blues', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Model Accuracy Heatmap\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
